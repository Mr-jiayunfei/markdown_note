# 数据降维之等度量映射

## 数据降维含义

含义：数据降维的基本出发点是在尽量保留原始数据特征的前提下，降低参与建模的维度数。在降维过程中，无论未被表现出来的特征是噪音还是正常分布，这部分信息都无法参与建模。如果某些场景下需要所有数据集的完整特征，那么通常不选择降维。

## 数据降维好处

1 避免过度拟合

2 减少误导性数据，提高模型准确性

3 减少数据，算法训练更快

4 数据减少，节约存储空间

5 消除多余的特征和噪音

![image-20210110082836131](https://i.loli.net/2021/01/10/hymsrlqpuoESNLb.png)





## 数据降维分类

![image-20210110082857262](https://i.loli.net/2021/01/10/VMqShbtLI6WzA5u.png)



## 等度量映射

### 流形学习

​     传统的机器学习方法中，数据点和数据点之间的距离和映射函数都是定义在欧式空间中的，然而在实际情况中，这些数据点可能不是分布在欧式空间中的，因此传统欧式空间的度量难以用于真实世界的非线性数据，从而需要对数据的分布引入新的假设。流形学习假设所处理的数据点分布在嵌入于外维欧式空间的一个潜在的流形体上，或者说这些数据点可以构成这样一个潜在的流形体。

  在三维空间中流行体上点与点之间的距离不能使用传统的欧式空间的距离来计算，而应该用测地线距离代表这两个点的实际距离。蓝色虚线为两个点的欧式距离，蓝色实线为两个点的测地线距离。但是测地线距离也不好测量，因此我们采用另一种路径近似代表测地线距离。

![image-20210110083003268](https://i.loli.net/2021/01/10/FtU1Bl29cQ8aNo5.png)

Euclidean Distance:欧式距离

Geodesic Distance: 测地线距离

![image-20210110083022647](https://i.loli.net/2021/01/10/kzX7KmyjABDUuft.png)

我们构建一个连通图，其中每个点只和距离这个点最近的k个点直接连接，和其他的点不直接连接。这样我们可以构建邻接矩阵，进而求出图中任意两个点的最短路径，代替测地线距离。蓝色线代表两个点之间的测地线距离，红色线代表图中两点的最短路径，两者距离相近，因此我们使用后者替代前者。

![image-20210110083037450](https://i.loli.net/2021/01/10/VqO9NoQZjirAHSJ.png)



### 最短路径

使用Djikstra算法寻找X到Y的最短路径

**X-> B-> H-> G-> Y**

![image-20210110085549008](https://i.loli.net/2021/01/10/PImpcvwWKkGXgNZ.png)

### 什么时候使用Isomap？

Isomap is better than linear methods when dealing with almost all types of real image and motion tracking 



### 算法流程:

![image-20210110085636457](https://i.loli.net/2021/01/10/vkPhuzRer9xBjHy.png)



### 测试用例1

1 加载数据集，并且可视化显示

![image-20210110085705282](https://i.loli.net/2021/01/10/DlgbWiOG5xCI7qB.png)



2 调用PCA算法

![image-20210110085716227](https://i.loli.net/2021/01/10/EurjsoLIHqJTUAP.png)



3 调用Isomap算法

![image-20210110085729283](https://i.loli.net/2021/01/10/rAQniEp4FNqHuLc.png)



### 测试用例2(人脸识别数据集)

![image-20210110085802079](https://i.loli.net/2021/01/10/IjCxDc7oz5UJafl.png)

![image-20210110085821756](https://i.loli.net/2021/01/10/aHXCu7Q86oUBisz.png)

![image-20210110085836441](https://i.loli.net/2021/01/10/qHuAyN3DMioY7Gl.png)

![image-20210110085850145](https://i.loli.net/2021/01/10/XRxWnFa7qEYAj2N.png)





![image-20210110085859926](https://i.loli.net/2021/01/10/EmpZ4T8YCqdWyfo.png)

### 参考资料

https://benalexkeen.com/isomap-for-dimensionality-reduction-in-python/



https://towardsdatascience.com/decomposing-non-linearity-with-isomap-32cf1e95a483



[https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)



http://benalexkeen.com/implementing-djikstras-shortest-path-algorithm-with-python/

# 随机抽样和分层抽样

在将样本数据分成训练集和测试集的时候，应当谨慎地考虑一下是采用纯随机抽样，还是分层抽样。通常，数据集如果足够大，纯随机抽样的方式，将样本数据分成两个子集是没有太大的问题。如果不是，纯随机抽样肯可能会导致抽样数据偏差，影响训练效果，降低预测模型预测的准确性。设想调查公司需要做1000份抽样调查，调查的问题和性别可能有较大的相关性。如果想让调查结果代表全国男性和女性对这些问题的看法，假设全国人口男女比例大致为60：40，那么在1000份问卷也应当尽量保持男女比例达到同样的比例，即参加问卷调查的男女数差不多是600和400。这个就是分层抽样。如果参加问卷的男女数比例很不一样，比如女性占到了60%或更多，那么调查结伦就会出现重大偏差。

使用sklearn.model_selection.train_test_split，参数stratify即用来指定按照某一特征进行分层抽样，生成训练集和测试集



# read_csv

pd.read_csv('data.csv',encoding = "utf-8",header = 0,names = range(0,50),index_col=0)

- header = 0是默认情况（即不标明，默认就是header = 0），表示以数据的第一行为列索引。
- encoding = "utf-8"表明以utf-8为编码规则。
- names = range(0,50))表示以[0....49]为列索引的名字
- index_col=0表示以原有数据的第一列(索引为0)当作行索引。



# pytorch之断点续训

```
https://www.zhihu.com/search?type=content&q=%E8%AE%AD%E7%BB%83%20%E6%96%AD%E7%82%B9
```



# epoch，batch size ，iteration

epoch指的是次数，epoch = 10 指的是把整个数据集丢进神经网络训练10次。 batch size 指的是数据的个数，batch size = 10 指的是每次扔进神经网络训练的数据是10个。iteration同样指的是次数，iteration = 10 指的是把整个数据集分成10次扔进神经网络。